{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/rnshishir/deepmed/CPRD_Cut22/debug'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.insert(0, '/home/rnshishir/deepmed/CPRD_Cut22/')\n",
    "import shutil\n",
    "from utils.yaml_act import yaml_load\n",
    "from utils.arg_parse import arg_paser\n",
    "from CPRD.config.spark import spark_init, read_parquet\n",
    "import pyspark.sql.functions as F\n",
    "from CPRD.functions import tables, merge, cohort_select\n",
    "from CPRD.functions import merge\n",
    "from CPRD.functions.modalities import *\n",
    "\n",
    "from utils.utils import save_obj\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def vocabCreate(CountsICD, num):\n",
    "    dic={}\n",
    "    dic['token2idx']={}\n",
    "    dic['idx2token']={}\n",
    "    dic['token2idx']['MASK']=4\n",
    "    dic['token2idx']['CLS']=3\n",
    "    dic['token2idx']['SEP']=2\n",
    "    dic['token2idx']['UNK']=1\n",
    "    dic['token2idx']['PAD']=0\n",
    "    dic['idx2token'][4]='MASK'\n",
    "    dic['idx2token'][3]='CLS'\n",
    "    dic['idx2token'][2]='SEP'\n",
    "    dic['idx2token'][1]='UNK'\n",
    "    dic['idx2token'][0]='PAD'\n",
    "    i=5\n",
    "    for x in CountsICD:\n",
    "        if CountsICD[x]>num:\n",
    "            dic['token2idx'][x]=i\n",
    "            dic['idx2token'][i]=x\n",
    "            i=i+1\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rnshishir/deepmed/CPRD_Cut22/utils/yaml_act.py:6: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  cfg = yaml.load(ymlfile)\n"
     ]
    }
   ],
   "source": [
    "args = dotdict({'params': '/home/rnshishir/deepmed/CPRD_Cut22/config/config.yaml'})\n",
    "params = yaml_load(args.params)\n",
    "spark_params = params['pyspark']\n",
    "spark = spark_init(spark_params)\n",
    "file = params['file_path']\n",
    "# data_params = params['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 'medication.main',\n",
       " 'pyspark': {'temp': '/home/rnshishir/sparkNewT',\n",
       "  'pyspark_env': '/home/rnshishir/anaconda3/envs/pyspark37/bin/python3.7'},\n",
       " 'file_path': {'clinical': '/home/workspace/datasets/cprd/cprd2021/*/*_Observation_*',\n",
       "  'patient': '/home/workspace/datasets/cprd/cprd2021/*/*_Patient_*',\n",
       "  'problem': '/home/workspace/datasets/cprd/cprd2021/*/*_Problem_*',\n",
       "  'therapy': '/home/workspace/datasets/cprd/cprd2021/*/*_DrugIssue_*',\n",
       "  'referral': '/home/workspace/datasets/cprd/cprd2021/*/*_Referral_*',\n",
       "  'practice': '/home/workspace/datasets/cprd/cprd2021/*/*_Practice_*',\n",
       "  'consultation': '/home/workspace/datasets/cprd/cprd2021/*/*_Consultation_*',\n",
       "  'staff': '/home/workspace/datasets/cprd/cprd2021/*/*_Staff*',\n",
       "  'diagnosis_hes': '/home/workspace/datasets/cprd/cprd2021/linkage/20_095_Results/*/Aurum_linked/Final/hes_diagnosis_hosp_20_095_DM.txt',\n",
       "  'proc_hes': '/home/workspace/datasets/cprd/cprd2021/linkage/20_095_Results/*/Aurum_linked/Final/hes_procedures_epi_20_095_DM.txt',\n",
       "  'op_hes': '/home/workspace/datasets/cprd/cprd2021/linkage/20_095_Results/*/Aurum_linked/Final/hesop_clinical_20_095_DM.txt',\n",
       "  'death': '/home/workspace/datasets/cprd/cprd2021/linkage/20_095_Results/*/Aurum_linked/Final/death_patient_20_095_DM.txt',\n",
       "  'eligible': '/home/workspace/datasets/cprd/cprd2021/linkage_eligibility_Set21/linkage_eligibility*',\n",
       "  'med2read': '/home/workspace/datasets/cprd/cprd2021/202102_lookups/202102_EMISMedicalDictionary.txt',\n",
       "  'med2sno': '/home/workspace/datasets/cprd/cprd2021/202102_lookups/202102_EMISMedicalDictionary.txt',\n",
       "  'medicalDict': '/home/shared/shishir/AurumOut/Dicts/MedicalDictionary/CompiledDict/',\n",
       "  'uptodateDict': '/home/workspace/datasets/cprd/cprd2021/linkage/20_095_Results/Documentation/*/linkage_coverage_dictv.pkl',\n",
       "  'prod2bnf': '/home/workspace/datasets/cprd/cprd2021/202102_lookups/202102_EMISProductDictionary.txt',\n",
       "  'prod2bnf_vtm': '/home/shared/shishir/AurumOut/Dicts/meds_medcount_full.parquet',\n",
       "  'read2icd': '/home/workspace/datasets/cprd/cuts/02_cprd2015/3_Lookups/final_mapping.csv',\n",
       "  'sno2icd': '/home/shared/shishir/AurumOut/Dicts/snomed2icd.parquet',\n",
       "  'imd': '/home/workspace/datasets/cprd/cprd2021/linkage/20_095_Results/*/Aurum_linked/Final/patient_imd2015_20_095.txt',\n",
       "  'PhenoMaps': '/home/shared/shishir/AurumOut/Dicts/compiled_MD_23Nov2023',\n",
       "  'diag2keep': '/home/shared/shishir/AurumOut/Dicts/diagcodes2keep.parquet',\n",
       "  'hes_patient': '/home/workspace/datasets/cprd/cprd2021/linkage/20_095_Results/*/Aurum_linked/Final/hes_patient_20_095_DM.txt'},\n",
       " 'params': {'bnf_mapping': True, 'time_range': [1985, 2022]},\n",
       " 'save_file': 'medication.parquet'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# params have each of the individual files for data processing and other things like location of medical dict etc\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from CPRD.functions import tables, merge\n",
    "from CPRD.config.utils import *\n",
    "import CPRD.base.table as cprd_table\n",
    "import pandas as pd\n",
    "from CPRD.functions.modalities import *\n",
    "\n",
    "from CPRD.functions.Causal import *\n",
    "from pyspark.sql import Window\n",
    "from CPRD.functions.MedicalDictionary import *\n",
    "from CPRD.config.utils import *\n",
    "from CPRD.config.spark import read_parquet, read_txtzip, read_txt\n",
    "from CPRD.functions.cohort_select_causal import *\n",
    "from CPRD.functions.predictor_extractor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hes_apc': ['01/04/1997', '31/10/2020'],\n",
       " 'hes_acp': ['01/10/1997', '31/03/2008'],\n",
       " 'hes_ccare': ['01/04/2008', '31/10/2020'],\n",
       " 'hes_op': ['01/04/2003', '31/10/2020'],\n",
       " 'hes_ae': ['01/04/2007', '31/03/2020'],\n",
       " 'hes_did': ['01/04/2012', '31/10/2020'],\n",
       " 'ons_death': ['02/01/1998', '16/11/2020'],\n",
       " 'ncras_cr': ['01/01/1990', '31/12/2018'],\n",
       " 'ncras_sact': ['01/01/2014', '31/12/2018'],\n",
       " 'ncras_rtds': ['01/04/2012', '31/12/2018']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict4recs = load_obj( '/home/workspace/datasets/cprd/cprd2021/linkage/20_095_Results/Documentation/Set 21/linkage_coverage_dictv')\n",
    "\n",
    "dict4recs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hes outpatient (OP) extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cvt_datestr2time(df, col, year_first=True):\n",
    "    \"\"\"\n",
    "    convert column from string to date type\n",
    "\n",
    "    format: 1993/01/07\n",
    "    \"\"\"\n",
    "    return F.to_date(df[col] ,\"yyyyMMdd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "def read_txt2(sc, sqlContext, path):\n",
    "    \"\"\"Reads from a TXT file and returns a PySpark DataFrame, dropping malformed rows.\"\"\"\n",
    "    \n",
    "    # Read the file\n",
    "    file = sc.textFile(path)\n",
    "\n",
    "    # Extract header\n",
    "    head = file.first()\n",
    "    header_cols = head.split(\"\\t\")  # Define expected column names\n",
    "    print(header_cols)\n",
    "    # Remove header from the content\n",
    "    content = file.filter(lambda line: line != head).map(lambda k: k.split(\"\\t\"))\n",
    "    \n",
    "    # Ensure each row has the correct number of columns (DROPMALFORMED logic)\n",
    "    valid_rows = content.filter(lambda row: len(row) == len(header_cols))\n",
    "    \n",
    "    # Convert to Row objects\n",
    "    row_rdd = valid_rows.map(lambda row: Row(**dict(zip(header_cols, row))))\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = sqlContext.createDataFrame(row_rdd)\n",
    "    print(df.columns)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(sc, sqlContext, path):\n",
    "    \"\"\"read from txt to pyspark dataframe\"\"\"\n",
    "    file = sc.textFile(path)\n",
    "    head = file.first()\n",
    "    content = file.filter(lambda line: line != head).map(lambda k: k.split('\\t'))\n",
    "    df = sqlContext.createDataFrame(content, schema=head.split('\\t'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all files and join on important columns (patid AND attendkey identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['patid', 'attendkey', 'diag_01', 'diag_02', 'diag_03', 'diag_04', 'diag_05', 'diag_06', 'diag_07', 'diag_08', 'diag_09', 'diag_10', 'diag_11', 'diag_12', 'opertn_01', 'opertn_02', 'opertn_03', 'opertn_04', 'opertn_05', 'opertn_06', 'opertn_07', 'opertn_08', 'opertn_09', 'opertn_10', 'opertn_11', 'opertn_12', 'opertn_13', 'opertn_14', 'opertn_15', 'opertn_16', 'opertn_17', 'opertn_18', 'opertn_19', 'opertn_20', 'opertn_21', 'opertn_22', 'opertn_23', 'opertn_24', 'operstat', 'HES_yr']\n",
      "['HES_yr', 'attendkey', 'diag_01', 'diag_02', 'diag_03', 'diag_04', 'diag_05', 'diag_06', 'diag_07', 'diag_08', 'diag_09', 'diag_10', 'diag_11', 'diag_12', 'operstat', 'opertn_01', 'opertn_02', 'opertn_03', 'opertn_04', 'opertn_05', 'opertn_06', 'opertn_07', 'opertn_08', 'opertn_09', 'opertn_10', 'opertn_11', 'opertn_12', 'opertn_13', 'opertn_14', 'opertn_15', 'opertn_16', 'opertn_17', 'opertn_18', 'opertn_19', 'opertn_20', 'opertn_21', 'opertn_22', 'opertn_23', 'opertn_24', 'patid']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[patid: string, attendkey: string, HES_yr: string, diag_01: string, diag_02: string, diag_03: string, diag_04: string, diag_05: string, diag_06: string, diag_07: string, diag_08: string, diag_09: string, diag_10: string, diag_11: string, diag_12: string, operstat: string, opertn_01: string, opertn_02: string, opertn_03: string, opertn_04: string, opertn_05: string, opertn_06: string, opertn_07: string, opertn_08: string, opertn_09: string, opertn_10: string, opertn_11: string, opertn_12: string, opertn_13: string, opertn_14: string, opertn_15: string, opertn_16: string, opertn_17: string, opertn_18: string, opertn_19: string, opertn_20: string, opertn_21: string, opertn_22: string, opertn_23: string, opertn_24: string, eventdate: date]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hesopclin = read_txt2(spark.sc, spark.sqlContext, '/home/workspace/datasets/cprd/cprd2021/linkage/20_095_Results/*/Aurum_linked/Final/hesop_clinical_20_095_DM.txt')\n",
    "hesfull = read_txt(spark.sc, spark.sqlContext, '/home/workspace/datasets/cprd/cprd2021/linkage/20_095_Results/aurum_hesapc_op_appttable.txt')\n",
    "hesfull =hesfull .withColumn('eventdate', F.concat(F.col('apptdate').substr(7, 4), F.col('apptdate').substr(4, 2), F.col('apptdate').substr(1, 2)))\n",
    "hesfull= hesfull.drop('apptdate')\n",
    "hesfull = hesfull.withColumn('eventdate', cvt_datestr2time(hesfull, 'eventdate'))\n",
    "hesopclin =hesopclin.join(hesfull, ['patid','attendkey'],'left')\n",
    "hesopclin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiate procedures and diags in the OP setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in [ \n",
    "    'opertn_01',\n",
    " 'opertn_02',\n",
    " 'opertn_03',\n",
    " 'opertn_04',\n",
    " 'opertn_05',\n",
    " 'opertn_06',\n",
    " 'opertn_07',\n",
    " 'opertn_08',\n",
    " 'opertn_09',\n",
    " 'opertn_10',\n",
    " 'opertn_11',\n",
    " 'opertn_12',\n",
    " 'opertn_13',\n",
    " 'opertn_14',\n",
    " 'opertn_15',\n",
    " 'opertn_16',\n",
    " 'opertn_17',\n",
    " 'opertn_18',\n",
    " 'opertn_19',\n",
    " 'opertn_20',\n",
    " 'opertn_21',\n",
    " 'opertn_22',\n",
    " 'opertn_23',\n",
    " 'opertn_24']:\n",
    "    hesopclin = hesopclin.withColumn(x, F.concat(F.lit(\"PROC_\"), F.col(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep only eligible period of \"clean\" records for HES OP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hesopclin = hesopclin.withColumn('goodstart', F.to_date(F.lit(DICT2KEEP['hes_op'][0]), 'dd/MM/yyyy')) \\\n",
    "    .withColumn('goodend', F.to_date(F.lit(DICT2KEEP['hes_op'][1]), 'dd/MM/yyyy'))\n",
    "\n",
    "hesopclin = hesopclin.filter(F.col('eventdate') >= F.col('goodstart')).filter(F.col('eventdate') < F.col('goodend'))\n",
    "hesopclin =hesopclin.drop('goodend').drop('goodstart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collapse all various diags/operations into a single column - expensive process..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep4_rmdot_dash_func(x):\n",
    "    x= x.replace(\".\", \"\")\n",
    "    x= x.replace(\"-\", \"\")\n",
    "    out = x\n",
    "    if 'PROC_' in x:\n",
    "        out = out[:9]\n",
    "    else:\n",
    "        out = out[:4]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[patid: string, attendkey: string, HES_yr: string, diag_01: string, diag_02: string, diag_03: string, diag_04: string, diag_05: string, diag_06: string, diag_07: string, diag_08: string, diag_09: string, diag_10: string, diag_11: string, diag_12: string, operstat: string, opertn_01: string, opertn_02: string, opertn_03: string, opertn_04: string, opertn_05: string, opertn_06: string, opertn_07: string, opertn_08: string, opertn_09: string, opertn_10: string, opertn_11: string, opertn_12: string, opertn_13: string, opertn_14: string, opertn_15: string, opertn_16: string, opertn_17: string, opertn_18: string, opertn_19: string, opertn_20: string, opertn_21: string, opertn_22: string, opertn_23: string, opertn_24: string, eventdate: date]\n",
      "DataFrame[patid: string, code: array<string>, eventdate: date]\n"
     ]
    }
   ],
   "source": [
    "cause_cols = ['diag_01',\n",
    " 'diag_02',\n",
    " 'diag_03',\n",
    " 'diag_04',\n",
    " 'diag_05',\n",
    " 'diag_06',\n",
    " 'diag_07',\n",
    " 'diag_08',\n",
    " 'diag_09',\n",
    " 'diag_10',\n",
    " 'diag_11',\n",
    " 'diag_12',\n",
    " 'opertn_01',\n",
    " 'opertn_02',\n",
    " 'opertn_03',\n",
    " 'opertn_04',\n",
    " 'opertn_05',\n",
    " 'opertn_06',\n",
    " 'opertn_07',\n",
    " 'opertn_08',\n",
    " 'opertn_09',\n",
    " 'opertn_10',\n",
    " 'opertn_11',\n",
    " 'opertn_12',\n",
    " 'opertn_13',\n",
    " 'opertn_14',\n",
    " 'opertn_15',\n",
    " 'opertn_16',\n",
    " 'opertn_17',\n",
    " 'opertn_18',\n",
    " 'opertn_19',\n",
    " 'opertn_20',\n",
    " 'opertn_21',\n",
    " 'opertn_22', \n",
    " 'opertn_23',\n",
    " 'opertn_24',\n",
    "             ]\n",
    "\n",
    "cause_cols = [F.col(each) for each in cause_cols]\n",
    "hesopclin = hesopclin.withColumn(\"code\", F.array(cause_cols)).select(['patid', 'code', 'eventdate'])\n",
    "\n",
    "keep4_ = F.udf(lambda x: keep4_rmdot_dash_func(x))\n",
    "\n",
    "hesopclin = hesopclin.withColumn('code', F.explode('code')) \\\n",
    "    .withColumn('code', keep4_('code')) \n",
    "\n",
    "hesopclin = hesopclin.filter(F.col('code')!='PROC_')\n",
    "hesopclin =  hesopclin.filter((F.col('eventdate') != '') & (F.col('code') != ''))\n",
    "hesopclin = hesopclin.filter((F.length(F.col(\"code\")) == 9) | (F.length(F.col(\"code\")) == 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra filtering by last X in code as often a wildcard character and nonsense procedure codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lastXrem(x):\n",
    "    out = x\n",
    "    if x  == x[-1] or 'X' == x[-1]:\n",
    "        out = out[:-1]\n",
    "    \n",
    "    return out\n",
    "lastX = F.udf(lambda x: lastXrem(x))\n",
    "hesopclin=hesopclin.withColumn('code', lastX('code')) \n",
    "hesopclin =hesopclin.dropDuplicates()\n",
    "hesopclin =hesopclin.filter(F.col ('code')!= \"PROC_XXX\")\n",
    "hesopclin = hesopclin.filter(\n",
    "    (F.col(\"code\").contains(\"PROC_X99\")) ==False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hesopclin.write.parquet('/home/shared/shishir/AurumOut/rawDat/HESOP_DiagProcClean.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "hesopclin= read_parquet( spark.sqlContext,'/home/shared/shishir/AurumOut/rawDat/HESOP_DiagProcClean.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 862,438,748 total records for HES OP\n",
    "# 24,916,264 unique patient data records in HES OP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hesopclin= read_parquet( spark.sqlContext,'/home/shared/shishir/AurumOut/rawDat/HESOP_DiagProcClean.parquet')\n",
    "# hesopclin = hesopclin.groupBy('code').count()\n",
    "# hesopclin.write.parquet('/home/shared/shishir/AurumOut/rawDat/HESOP_DiagProcClean_counts.parquet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    8579\n",
       "9    8222\n",
       "3     240\n",
       "Name: leng, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hesopclincounts= pd.read_parquet( '/home/shared/shishir/AurumOut/rawDat/HESOP_DiagProcClean_counts.parquet')\n",
    "hesopclincounts['leng'] = hesopclincounts.code.apply(lambda x: len(x))\n",
    "hesopclincounts.leng.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>count</th>\n",
       "      <th>leng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5548</th>\n",
       "      <td>R69</td>\n",
       "      <td>641718135</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7148</th>\n",
       "      <td>PROC_X621</td>\n",
       "      <td>19151211</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7149</th>\n",
       "      <td>PROC_Y981</td>\n",
       "      <td>11450590</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6053</th>\n",
       "      <td>PROC_X622</td>\n",
       "      <td>8770521</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290</th>\n",
       "      <td>PROC_C873</td>\n",
       "      <td>4505324</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14862</th>\n",
       "      <td>PROC_U216</td>\n",
       "      <td>3206067</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9872</th>\n",
       "      <td>PROC_X654</td>\n",
       "      <td>3025998</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14483</th>\n",
       "      <td>PROC_U263</td>\n",
       "      <td>2895373</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8599</th>\n",
       "      <td>PROC_Z941</td>\n",
       "      <td>2317505</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12666</th>\n",
       "      <td>PROC_X623</td>\n",
       "      <td>2316563</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            code      count  leng\n",
       "5548         R69  641718135     3\n",
       "7148   PROC_X621   19151211     9\n",
       "7149   PROC_Y981   11450590     9\n",
       "6053   PROC_X622    8770521     9\n",
       "1290   PROC_C873    4505324     9\n",
       "14862  PROC_U216    3206067     9\n",
       "9872   PROC_X654    3025998     9\n",
       "14483  PROC_U263    2895373     9\n",
       "8599   PROC_Z941    2317505     9\n",
       "12666  PROC_X623    2316563     9"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hesopclincounts.sort_values('count', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of proc:  8222\n",
      "number of diag:  8819\n"
     ]
    }
   ],
   "source": [
    "print('number of proc: ', len(hesopclincounts[hesopclincounts.leng==9]))\n",
    "print('number of diag: ', len(hesopclincounts[hesopclincounts.leng!=9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark3.7",
   "language": "python",
   "name": "pyspark37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
